FROM llama3.1
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 0
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 131072
# num_predict	Maximum number of tokens to predict when generating text. (Default: 128, -1=infinite generation, -2 = fill context)
PARAMETER num_predict 128
# top_k	Reduces the probability of generating nonsense.
#    A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
PARAMETER top_k	5
# top_p	Works together with top-k. A higher value (e.g., 0.95) 
#   will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
PARAMETER top_p .2
# sets a custom system message to specify the behavior of the chat assistant
System "You are an assistant that provides concise, clear answers. Do not add unnecessary information or ramble. Stick to the core of the user's question."